{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bda93ba",
   "metadata": {},
   "source": [
    "## Group No 048\n",
    "\n",
    "## Group Member Names:\n",
    "1. Bhavesh Pandey (2020SC04857)\n",
    "2. Nimish Pandey (2020SC04328)\n",
    "3. Harsh Vashisht (2020SC04565)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d80c60",
   "metadata": {},
   "source": [
    "# 1. Business Understanding\n",
    " \n",
    "Students are expected to identify a classification problem of your choice. You have to detail the Business Understanding part of your problem under this heading which basically addresses the following questions.\n",
    " \n",
    "   1. What is the problem that you are trying to solve?\n",
    "   2. What data do you need to answer the above problem?\n",
    "   3. What are the different sources of data?    \n",
    "   4. What kind of analytics task are you performing?\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3298e121",
   "metadata": {},
   "source": [
    "--------------Type the answers below this line-------------- \n",
    "\n",
    "1. We are trying to predict the next day stock price of all nifty 50 stocks based on the previous year data.\n",
    "\n",
    "2. Following data are needed to predict the stock price:\n",
    "    1. High Price of the day.\n",
    "    2. Open Price of the day.\n",
    "    3. Close Price of the day.\n",
    "    4. Low Price of the day.\n",
    "    5. Adjusted Close Price of the day.\n",
    "    6. 20 day moving average\n",
    "    7. 50 day moving average\n",
    "\n",
    "3. We use wikipedia to gather the information about the companies of nifty_50. and we use pandas_datareader to gather the data of last 6 years of all nifty_50 stocks.\n",
    "\n",
    "4. We use mlxtend for feature selection of each stock and apply linear regression model to predict the next day stock price."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc8e0cb",
   "metadata": {},
   "source": [
    "# 2. Data Acquisition\n",
    " \n",
    "For the problem identified by you, students have to find the data source themselves which should be a website which has the required data in it. You have to write Python crawler code to scrape data from the respective website rather than downloading ready-made dataset as such from sources like Kaggle etc. \n",
    "\n",
    "(Data downloaded from website like Kaggle will be awarded negative marks.)\n",
    "\n",
    "## 2.1 Code for scraping data from website\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6026a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pandas_datareader\n",
    "# pip install bs4\n",
    "# pip install mlxtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b51d895",
   "metadata": {},
   "outputs": [],
   "source": [
    "##---------Type the code below this line------------------##\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import csv\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import bs4 as bs\n",
    "import pandas_datareader as web\n",
    "import itertools\n",
    "from itertools import chain\n",
    "import datetime as dt\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import date\n",
    "from datetime import timedelta\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bdef81",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_tickers=[]\n",
    "\n",
    "def contains(s):\n",
    "    return not re.match(r'[^.]+$', s)\n",
    "\n",
    "def nifty50_tickers():\n",
    "    link = 'https://en.wikipedia.org/wiki/NIFTY_50'\n",
    "    response = requests.get(link)\n",
    "    soup = bs.BeautifulSoup(response.text, 'lxml')\n",
    "    table = soup.find('table', {'class' : 'wikitable sortable'}, {'id' : 'constituents'})\n",
    "    tickers=[]\n",
    "    for row in table.findAll('tr')[1:]:\n",
    "        ticker = row.findAll('td')[1].text[:]\n",
    "        tickers.append(ticker)\n",
    "    for i in tickers:\n",
    "        if contains(i) != True:\n",
    "            final_tickers.append(i)\n",
    "            \n",
    "    return final_tickers\n",
    "\n",
    "nifty50_tickers()\n",
    "\n",
    "string=\".NS\"\n",
    "new_tickerlist = [x + string for x in final_tickers]\n",
    "\n",
    "start = date.today() - timedelta(days=2190)\n",
    "end = date.today()\n",
    "\n",
    "if not os.path.exists('Companies'):\n",
    "    os.makedirs('Companies')\n",
    "\n",
    "for ticker in new_tickerlist:\n",
    "    if not os.path.exists('Companies/{}.csv'.format(ticker)):\n",
    "        print(\"{} - Fetching Data... \".format(ticker))\n",
    "        df = web.DataReader(ticker, 'yahoo', start, end)\n",
    "        df.to_csv('Companies/{}.csv'.format(ticker))\n",
    "    else:\n",
    "        print(\"{} - Already Fetched.!\".format(ticker))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49530d0c",
   "metadata": {},
   "source": [
    "## 2.2 Code for converting the above scraped data into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f4c171",
   "metadata": {},
   "outputs": [],
   "source": [
    "##---------Type the code below this line------------------##\n",
    "main_df = pd.DataFrame()\n",
    "\n",
    "for ticker in new_tickerlist:\n",
    "    df = pd.read_csv('Companies/{}.csv'.format(ticker))\n",
    "    df.set_index('Date', inplace=True)\n",
    "    df.rename(columns = {'Close': ticker}, inplace=True)\n",
    "    df.drop(columns = ['Open', 'High', 'Low', 'Adj Close', 'Volume'], inplace=True)\n",
    "    if main_df.empty:\n",
    "        main_df = df\n",
    "    else:\n",
    "        main_df = main_df.join(df, how='outer')\n",
    "        \n",
    "main_df = main_df.reset_index()\n",
    "main_df.set_index('Date', inplace=True)\n",
    "Dataframe = pd.concat([main_df.tail(1)])\n",
    "\n",
    "high = Dataframe.copy()\n",
    "open1 = Dataframe.copy()\n",
    "low = Dataframe.copy()\n",
    "close = Dataframe.copy()\n",
    "\n",
    "high.loc[:] = np.nan\n",
    "open1.loc[:] = np.nan\n",
    "low.loc[:] = np.nan\n",
    "close.loc[:] = np.nan\n",
    "\n",
    "Datelist=[]\n",
    "\n",
    "if not os.path.exists(\"Companies_data\"):\n",
    "    os.makedirs(\"Companies_data\")\n",
    "\n",
    "for ticker in new_tickerlist:\n",
    "    with open(\"Companies_data/{}.csv\".format(ticker), 'w') as f:\n",
    "        pass\n",
    "    df = pd.read_csv('Companies/{}.csv'.format(ticker))\n",
    "    data1 = pd.concat([df.tail(2)])\n",
    "    data1.drop(df.tail(1).index,inplace=True)\n",
    "    data1.to_csv(\"Companies_data\\{}.csv\".format(ticker), header=True, index=False)\n",
    "    with open(\"Companies_data\\{}.csv\".format(ticker)) as f:\n",
    "        companies_csv = csv.DictReader(f)\n",
    "        for row in companies_csv:\n",
    "            Date = row['Date']\n",
    "            HighPrice = float(float(row['High']))\n",
    "            OpenPrice = float(float(row['Open']))\n",
    "            LowPrice = float(float(row['Low']))\n",
    "            ClosePrice = float(float(row['Close']))\n",
    "        H = HighPrice\n",
    "        O = OpenPrice\n",
    "        L = LowPrice\n",
    "        C = ClosePrice\n",
    "    Datelist.append(Date)\n",
    "    high[ticker].fillna(H, inplace=True)\n",
    "    open1[ticker].fillna(O, inplace=True)\n",
    "    low[ticker].fillna(L, inplace=True)\n",
    "    close[ticker].fillna(C, inplace=True)\n",
    "    \n",
    "newdata = pd.DataFrame(columns=['Companies', '200_Moving_AVG', '50_Moving_AVG'])\n",
    "newdata['Companies'] = new_tickerlist\n",
    "mva50all, mva200all = ([] for i in range(2))\n",
    "\n",
    "for tickers in new_tickerlist:\n",
    "    df = pd.read_csv('Companies/{}.csv'.format(tickers))\n",
    "    mva50 = round(df['Close'].rolling(window=50, min_periods=0).mean(), 3)\n",
    "    mva200 = round(df['Close'].rolling(window=200, min_periods=0).mean(), 3)\n",
    "    df['50_mVA'] = mva50\n",
    "    df['200_mvA'] = mva200\n",
    "    df.to_csv('Companies/{}.csv'.format(tickers), index=False)       \n",
    "    mva50all.append(mva50.iloc[-1])\n",
    "    mva200all.append(mva200.iloc[-1])\n",
    "    \n",
    "newdata['200_Moving_AVG'] = mva200all\n",
    "newdata['50_Moving_AVG'] = mva50all\n",
    "\n",
    "high1,low1,open2,close1 = ([] for i in range(4))\n",
    "    \n",
    "for (colname, colval) in low.iteritems():\n",
    "    low1.append(round(float(colval), 3))\n",
    "    \n",
    "for (colname, colval) in open1.iteritems():\n",
    "    open2.append(round(float(colval), 3))\n",
    "\n",
    "for (colname, colval) in high.iteritems():\n",
    "    high1.append(round(float(colval), 3))\n",
    "    \n",
    "for (colname, colval) in close.iteritems():\n",
    "    close1.append(round(float(colval), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1fea4d",
   "metadata": {},
   "source": [
    "## 2.3 Confirm the data has been correctly by displaying the first 5 and last 5 records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624e6c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "##---------Type the code below this line------------------##\n",
    "for tickers in new_tickerlist:\n",
    "    df = pd.read_csv('Companies/{}.csv'.format(ticker))\n",
    "    print(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500dfc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tickers in new_tickerlist:\n",
    "    df = pd.read_csv('Companies/{}.csv'.format(ticker))\n",
    "    print(df.tail(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb84fc56",
   "metadata": {},
   "source": [
    "## 2.4 Display the column headings, statistical information, description and statistical summary of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086ad28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##---------Type the code below this line------------------##\n",
    "for tickers in new_tickerlist:\n",
    "    df = pd.read_csv('Companies/{}.csv'.format(ticker))\n",
    "    print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812edb18",
   "metadata": {},
   "source": [
    "## 2.5 Write your observations from the above. \n",
    "1. Size of the dataset\n",
    "2. What type of data attributes are there?\n",
    "3. Is there any null data that has to be cleaned?\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "60d80d2f",
   "metadata": {},
   "source": [
    "--------------Type the answers below this line--------------\n",
    "\n",
    "1. 50 Dataset of nifty 50 Stocks. 1 Stock dataset contains 1481 Rows and 9 Columns.\n",
    "2. All attributes are numerical.\n",
    "3. No Null values found in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102e0e36",
   "metadata": {},
   "source": [
    "# 3. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcb953b",
   "metadata": {},
   "source": [
    "## 3.1 Display how many unique values are present in each attribute\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5b960e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##---------Type the code below this line------------------##\n",
    "for tickers in new_tickerlist:\n",
    "    df = pd.read_csv('Companies/{}.csv'.format(tickers))\n",
    "    print(df.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fdebf8",
   "metadata": {},
   "source": [
    "## 3.2 Check for the presence of duplicate data, identify the attributes with duplicate data, report the attributes. Mention the method adopted to remove duplicate data if present. Report the results again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86d5b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##---------Type the code below this line------------------##\n",
    "for ticker in new_tickerlist:\n",
    "    df = pd.read_csv('Companies/{}.csv'.format(ticker))\n",
    "    print(df.duplicated())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cec4fc",
   "metadata": {},
   "source": [
    "## 3.3 Show whether there are any missing values in each attribute. Report the same.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747250db",
   "metadata": {},
   "outputs": [],
   "source": [
    "##---------Type the code below this line------------------##\n",
    "for ticker in new_tickerlist:\n",
    "    df = pd.read_csv('Companies/{}.csv'.format(ticker))\n",
    "    print(df.isnull())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b4a34f",
   "metadata": {},
   "source": [
    "## 3.4 Clean the missing data using any imputation technique, mention the method used and again report the change after cleaning the data."
   ]
  },
  {
   "cell_type": "raw",
   "id": "38033c60",
   "metadata": {},
   "source": [
    "##---------Type the code below this line------------------##\n",
    "\n",
    "No missing value found in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33700253",
   "metadata": {},
   "source": [
    "## 3.5 Check if all the attributes are following the same format and are consistent. If not, report all such attributes and what inconsistencies are present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9124fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "##---------Type the code below this line------------------##\n",
    "for ticker in new_tickerlist:\n",
    "    df = pd.read_csv('Companies/{}.csv'.format(ticker))\n",
    "    print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c936608",
   "metadata": {},
   "source": [
    "## 3.6 Correct the data if there are inconsistencies from 3.5. Report or print the data after correction."
   ]
  },
  {
   "cell_type": "raw",
   "id": "f31dda4f",
   "metadata": {},
   "source": [
    "##---------Type the code below this line------------------##\n",
    "\n",
    "All attributes are in same format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793cd04b",
   "metadata": {},
   "source": [
    "## 3.7 Identify the target variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f28adc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "##---------Type the code below this line------------------##\n",
    "\n",
    "if not os.path.exists('DuplicateData'):\n",
    "    os.makedirs('DuplicateData')\n",
    "    \n",
    "for ticker in new_tickerlist:\n",
    "    df = pd.read_csv('Companies/{}.csv'.format(ticker), index_col=0)\n",
    "    days = 1\n",
    "    df['pred_cp'] = df['Adj Close'].shift(-days)\n",
    "    df['High'] = df['High'].shift(-days)\n",
    "    df['Low'] = df['Low'].shift(-days)\n",
    "    df['Open'] = df['Open'].shift(-days)\n",
    "    df['Close'] = df['Close'].shift(-days)\n",
    "    df.to_csv('Companies/{}.csv'.format(ticker), header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f9650a",
   "metadata": {},
   "source": [
    "## 3.8 Separate the data front the target such that the dataset is in the form of (X,y) or (Features, Label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bd5315",
   "metadata": {},
   "outputs": [],
   "source": [
    "##---------Type the code below this line------------------##\n",
    "for ticker in new_tickerlist:\n",
    "    df = pd.read_csv('Companies/{}.csv'.format(ticker))\n",
    "    df.dropna(inplace=True)\n",
    "    X = np.array(df[['Open', 'High', 'Low', 'Close', '50_mVA', '200_mvA']])\n",
    "    y = np.array(df['pred_cp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280c1f13",
   "metadata": {},
   "source": [
    "## 3.9 Discretize the target variable or perform one-hot encoding on the target or any other as and if required.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "627e19dc",
   "metadata": {},
   "source": [
    "##---------Type the code below this line------------------##\n",
    "\n",
    "** Not Required **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae0b5d2",
   "metadata": {},
   "source": [
    "# 4. Data Exploration using various plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186bf4d7",
   "metadata": {},
   "source": [
    "## 4.1 Scatter plot of each attribute with the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868d7b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "##---------Type the code below this line------------------##\n",
    "print(plt.scatter(X.T[0], y))\n",
    "print(plt.scatter(X.T[1], y))\n",
    "print(plt.scatter(X.T[2], y))\n",
    "print(plt.scatter(X.T[3], y))\n",
    "print(plt.scatter(X.T[4], y))\n",
    "print(plt.scatter(X.T[5], y))\n",
    "# Scatter plot of the last dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57150fdd",
   "metadata": {},
   "source": [
    "## 4.2 Pair plot of each attribute to identify the linear relationships among the attributes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3118eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "##---------Type the code below this line------------------##\n",
    "for ticker in new_tickerlist:\n",
    "    pp = pd.read_csv('Companies/{}.csv'.format(ticker))\n",
    "print(sns.pairplot(pp))\n",
    "# Pair Plot of the last dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b908288",
   "metadata": {},
   "source": [
    "## 4.3 Regression plots to identify the linear relationship between each attribute with the target variable. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a421fb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ticker in new_tickerlist:\n",
    "    rg1 = pd.read_csv('Companies/{}.csv'.format(ticker))\n",
    "print(sns.regplot(x='High', y='pred_cp', data=rg1))\n",
    "# Regression Plot of the last dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0690f707",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ticker in new_tickerlist:\n",
    "    rg2 = pd.read_csv('Companies/{}.csv'.format(ticker))\n",
    "print(sns.regplot(x='Open', y='pred_cp', data=rg2))\n",
    "# Regression Plot of the last dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7344c18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ticker in new_tickerlist:\n",
    "    rg3 = pd.read_csv('Companies/{}.csv'.format(ticker))\n",
    "print(sns.regplot(x='Close', y='pred_cp', data=rg3))\n",
    "# Regression Plot of the last dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a326a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ticker in new_tickerlist:\n",
    "    rg4 = pd.read_csv('Companies/{}.csv'.format(ticker))\n",
    "print(sns.regplot(x='Low', y='pred_cp', data=rg4))\n",
    "# Regression Plot of the last dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f45a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ticker in new_tickerlist:\n",
    "    rg5 = pd.read_csv('Companies/{}.csv'.format(ticker))\n",
    "print(sns.regplot(x='Adj Close', y='pred_cp', data=rg5))\n",
    "# Regression Plot of the last dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ed80c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ticker in new_tickerlist:\n",
    "    rg6 = pd.read_csv('Companies/{}.csv'.format(ticker))\n",
    "print(sns.regplot(x='Volume', y='pred_cp', data=rg6))\n",
    "# Regression Plot of the last dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575f9e37",
   "metadata": {},
   "source": [
    "## 4.4 Can any other plot help to identify the optimal set of attributes that can be used for classification. The plot will be based on linear or nonlinear separations. If there is/are such plots, name them, explain why you think they can be helpful in the task and perform the plot as well.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8ed48a9e",
   "metadata": {},
   "source": [
    "##---------Type the code below this line------------------##\n",
    "\n",
    "** This is not a classification Problem **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbc82a1",
   "metadata": {},
   "source": [
    "# 5. Data Wrangling\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e57102",
   "metadata": {},
   "source": [
    "## 5.1 Display correlation heatmap of each attribute against the target and report which features are significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f20f541",
   "metadata": {},
   "outputs": [],
   "source": [
    "##---------Type the code below this line------------------##\n",
    "for ticker in new_tickerlist:\n",
    "    hm = pd.read_csv('Companies/{}.csv'.format(ticker), index_col=0)\n",
    "print(sns.heatmap(hm))\n",
    "# Heatmap of the last dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca214eb3",
   "metadata": {},
   "source": [
    "## 5.2 Univariate Filters – Identify top 5 significant features by evaluating each feature independently with respect to the target variable by exploring \n",
    "1. Mutual Information (Information Gain)\n",
    "2. Gini index\n",
    "3. Gain Ratio\n",
    "4. Chi-Squared test\n",
    "5. Fisher Score\n",
    "(From the above 5 you are required to use only any <b>three</b>) \n",
    "\n",
    "Write your observations from the results of each method and report the top 5 significant features for each of the above methods. Also plot a graph of significant features for each of them for better visualization."
   ]
  },
  {
   "cell_type": "raw",
   "id": "cdca806d",
   "metadata": {},
   "source": [
    "##---------Type the code below this line------------------##\n",
    "\n",
    "Measured values(k_feature, k_score, confidence) are displayed in final dataframe as below.\n",
    "\n",
    "** This is not a classification Problem **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f1173c",
   "metadata": {},
   "source": [
    "## 5.3 Train a “DecisionTreeClassifier” on the entire data and use the classifier to extract the top 5 significant features. Plot graph of significant features for better visualization."
   ]
  },
  {
   "cell_type": "raw",
   "id": "c6b30768",
   "metadata": {},
   "source": [
    "##---------Type the code below this line------------------##\n",
    "\n",
    "We use linear regresison to predict the target variable.\n",
    "\n",
    "** This is not a classification Problem **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040afed8",
   "metadata": {},
   "source": [
    "## 5.4 Using \"mlxtend\" library perform SequentialFeatureSelector to identify top 5 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7042235d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##---------Type the code below this line------------------##\n",
    "predictive_values,confidence1,kfeature,kscore = ([] for i in range(4))\n",
    "\n",
    "for ticker in new_tickerlist:\n",
    "    df = pd.read_csv('Companies/{}.csv'.format(ticker), index_col=0)\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    X = np.array(df[['Open', 'High', 'Low', 'Close', '50_mVA', '200_mvA']])\n",
    "    y = np.array(df['pred_cp'])\n",
    "\n",
    "    lab_enc = preprocessing.LabelEncoder()\n",
    "    encoded = lab_enc.fit_transform(y)\n",
    "    knn = KNeighborsClassifier(n_neighbors=2)\n",
    "\n",
    "    sfs = SFS(knn, k_features=5, forward=True, floating=False, verbose=2, scoring='accuracy', cv=0)\n",
    "\n",
    "    feature_names = ('Open', 'High', 'Low', 'Close', '50_mVA', '200mVA')\n",
    "    sfs = sfs.fit(X, encoded, custom_feature_names=feature_names)\n",
    "    kfeature.append(sfs.k_feature_idx_)\n",
    "    kscore.append(sfs.k_score_)\n",
    "    \n",
    "    X = preprocessing.scale(X)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=1)\n",
    "    clf = LinearRegression()\n",
    "    clf.fit(X_train, y_train)\n",
    "    confidence = clf.score(X_test, y_test)\n",
    "    confidence1.append(confidence)\n",
    "\n",
    "    X = X[:-days]\n",
    "    X_new = X[-days:]\n",
    "    prediction = clf.predict(X_new)\n",
    "    \n",
    "    predictive_values.append(float(np.ma.round(prediction, decimals=3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922ec85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataframe = pd.DataFrame(columns=['Date', \n",
    "                                         'Companies', \n",
    "                                         'Open', \n",
    "                                         'High', \n",
    "                                         'Low',\n",
    "                                         'Close',\n",
    "                                         '50d Moving AVG',\n",
    "                                         '200d Moving AVG',\n",
    "                                         'Predictive Close',\n",
    "                                         'Confidence',\n",
    "                                         'k_feature',\n",
    "                                         'k_score'])\n",
    "\n",
    "final_dataframe['Date'] = Datelist\n",
    "final_dataframe['Companies'] = new_tickerlist\n",
    "final_dataframe['Open'] = open2\n",
    "final_dataframe['High'] = high1\n",
    "final_dataframe['Low'] = low1\n",
    "final_dataframe['Close'] = close1\n",
    "final_dataframe['50d Moving AVG'] = mva50all\n",
    "final_dataframe['200d Moving AVG'] = mva200all\n",
    "final_dataframe['Predictive Close'] = predictive_values\n",
    "final_dataframe['Confidence'] = confidence1\n",
    "final_dataframe['k_feature'] = kfeature\n",
    "final_dataframe['k_score'] = kscore\n",
    "\n",
    "final_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdee342",
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree('Companies')\n",
    "shutil.rmtree('Companies_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb57940c",
   "metadata": {},
   "source": [
    "## 5.5 Conclude the top 3 significant features with necessary justifications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324448eb",
   "metadata": {},
   "source": [
    "--------------Type the answers below this line--------------\n",
    "\n",
    "In section 5.4 top 5 features has been selected which shows that for maximum number of companies feature set (0, 1, 3, 4, 5) is most useful. As per this following are the top 3 significant features:\n",
    "\n",
    "1. Open\n",
    "2. High\n",
    "3. Close\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
